{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "from vqaTools.vqa import VQA\n",
    "from vqaEvaluation.vqaEval import VQAEval\n",
    "import random\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "#sys.path.insert(0, '{}/PythonHelperTools/vqaTools'.format(dataDir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir\t\t='Data'\n",
    "versionType ='' # this should be '' when using VQA v2.0 dataset\n",
    "taskType    ='OpenEnded' # 'OpenEnded' only for v2.0. 'OpenEnded' or 'MultipleChoice' for v1.0\n",
    "dataType    ='abstract_v002'  # 'mscoco' only for v1.0. 'mscoco' for real and 'abstract_v002' for abstract for v1.0.\n",
    "dataSubType ='train2015'\n",
    "annFile     ='{0}/Annotations/{1}{2}_{3}_annotations.json'.format(dataDir, versionType, dataType, dataSubType)\n",
    "quesFile    ='{0}/Questions/{2}_{1}{3}_{4}_questions.json'.format(dataDir, versionType, taskType, dataType, dataSubType)\n",
    "imgDir \t\t= '{0}/Images/scene_img_{1}_{2}/'.format(dataDir, dataType, dataSubType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading VQA annotations and questions into memory...\n",
      "0:00:01.022297\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# initialize VQA api for QA annotations\n",
    "vqa = VQA(annFile, quesFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many children does the youngest woman have?\n",
      "Answer 1: 1\n",
      "Answer 2: 1\n",
      "Answer 3: 1\n",
      "Answer 4: 1\n",
      "Answer 5: 1\n",
      "Answer 6: 1\n",
      "Answer 7: 1\n",
      "Answer 8: 1\n",
      "Answer 9: 1\n",
      "Answer 10: 1\n"
     ]
    }
   ],
   "source": [
    "# load and display QA annotations for given question types\n",
    "\"\"\"\n",
    "All possible quesTypes for abstract and mscoco has been provided in respective text files in ../QuestionTypes/ folder.\n",
    "\"\"\"\n",
    "annIds = vqa.getQuesIds(quesTypes='how many');   \n",
    "anns = vqa.loadQA(annIds)\n",
    "randomAnn = random.choice(anns)\n",
    "vqa.showQA([randomAnn])\n",
    "imgId = randomAnn['image_id']\n",
    "imgFilename = dataType + '_' + dataSubType + '_'+ str(imgId).zfill(12) + '.png'\n",
    "if os.path.isfile(imgDir + imgFilename):\n",
    "    I = io.imread(imgDir + imgFilename)\n",
    "    plt.imshow(I)\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Are they on a date?\n",
      "Answer 1: yes\n",
      "Answer 2: yes\n",
      "Answer 3: yes\n",
      "Answer 4: yes\n",
      "Answer 5: yes\n",
      "Answer 6: yes\n",
      "Answer 7: yes\n",
      "Answer 8: yes\n",
      "Answer 9: yes\n",
      "Answer 10: yes\n",
      "Question: Does the boy have everything he needs to build a sandcastle?\n",
      "Answer 1: no\n",
      "Answer 2: no\n",
      "Answer 3: no\n",
      "Answer 4: no\n",
      "Answer 5: no\n",
      "Answer 6: no\n",
      "Answer 7: no\n",
      "Answer 8: no\n",
      "Answer 9: no\n",
      "Answer 10: no\n"
     ]
    }
   ],
   "source": [
    "# load and display QA annotations for given answer types\n",
    "\"\"\"\n",
    "ansTypes can be one of the following\n",
    "yes/no\n",
    "number\n",
    "other\n",
    "\"\"\"\n",
    "annIds = vqa.getQuesIds(ansTypes='yes/no');   \n",
    "anns = vqa.loadQA(annIds)\n",
    "randomAnn = random.choice(anns)\n",
    "vqa.showQA([randomAnn])\n",
    "imgId = randomAnn['image_id']\n",
    "imgFilename = dataType + '_' + dataSubType + '_'+ str(imgId).zfill(12) + '.png'\n",
    "if os.path.isfile(imgDir + imgFilename):\n",
    "\tI = io.imread(imgDir + imgFilename)\n",
    "\tplt.imshow(I)\n",
    "\tplt.axis('off')\n",
    "\tplt.show()\n",
    "\n",
    "# load and display QA annotations for given images\n",
    "\"\"\"\n",
    "Usage: vqa.getImgIds(quesIds=[], quesTypes=[], ansTypes=[])\n",
    "Above method can be used to retrieve imageIds for given question Ids or given question types or given answer types.\n",
    "\"\"\"\n",
    "ids = vqa.getImgIds()\n",
    "annIds = vqa.getQuesIds(imgIds=random.sample(ids,5));  \n",
    "anns = vqa.loadQA(annIds)\n",
    "randomAnn = random.choice(anns)\n",
    "vqa.showQA([randomAnn])  \n",
    "imgId = randomAnn['image_id']\n",
    "imgFilename = dataType + '_' + dataSubType + '_'+ str(imgId).zfill(12) + '.png'\n",
    "if os.path.isfile(imgDir + imgFilename):\n",
    "\tI = io.imread(imgDir + imgFilename)\n",
    "\tplt.imshow(I)\n",
    "\tplt.axis('off')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading VQA annotations and questions into memory...\n",
      "0:00:00.649285\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Results do not correspond to current VQA set. Either results do not have predictions for all question ids in annotation file or there is atleast one question id that does not belong to the question ids in the annotation file.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-49d2016a2892>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# create vqa object and vqaRes object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mvqa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVQA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mannFile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquesFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mvqaRes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvqa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadRes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresFile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquesFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# create vqaEval object by taking vqa and vqaRes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\VQA-MS\\vqaTools\\vqa.py\u001b[0m in \u001b[0;36mloadRes\u001b[1;34m(self, resFile, quesFile)\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manns\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'results is not an array of objects'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[0mannsQuesIds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mann\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'question_id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mann\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mannsQuesIds\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetQuesIds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Results do not correspond to current VQA set. Either results do not have predictions for all question ids in annotation file or there is atleast one question id that does not belong to the question ids in the annotation file.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m         \u001b[1;31m#for ann in anns:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Results do not correspond to current VQA set. Either results do not have predictions for all question ids in annotation file or there is atleast one question id that does not belong to the question ids in the annotation file."
     ]
    }
   ],
   "source": [
    "# set up file names and paths\n",
    "versionType ='' # this should be '' when using VQA v2.0 dataset\n",
    "#taskType    ='OpenEnded' # 'OpenEnded' only for v2.0. 'OpenEnded' or 'MultipleChoice' for v1.0\n",
    "taskType    ='MultipleChoice' # 'OpenEnded' only for v2.0. 'OpenEnded' or 'MultipleChoice' for v1.0\n",
    "dataType    ='abstract_v002'  # 'mscoco' only for v1.0. 'mscoco' for real and 'abstract_v002' for abstract for v1.0. \n",
    "dataSubType ='val2015'\n",
    "annFile     ='{0}/Annotations/{1}{2}_{3}_annotations.json'.format(dataDir, versionType, dataType, dataSubType)\n",
    "quesFile    ='{0}/Questions/{2}_{1}{3}_{4}_questions.json'.format(dataDir, versionType, taskType, dataType, dataSubType)\n",
    "imgDir \t\t= '{0}/Images/scene_img_{1}_{2}/'.format(dataDir, dataType, dataSubType)\n",
    "resultType  ='fake'\n",
    "fileTypes   = ['results', 'accuracy', 'evalQA', 'evalQuesType', 'evalAnsType'] \n",
    "\n",
    "# An example result json file has been provided in './Results' folder.  \n",
    "\n",
    "[resFile, accuracyFile, evalQAFile, evalQuesTypeFile, evalAnsTypeFile] = ['{}/Results/{}{}_{}_{}_{}_{}.json'.format(dataDir, versionType, taskType, dataType, dataSubType, \\\n",
    "resultType, fileType) for fileType in fileTypes]  \n",
    "\n",
    "# create vqa object and vqaRes object\n",
    "vqa = VQA(annFile, quesFile)\n",
    "vqaRes = vqa.loadRes(resFile, quesFile)\n",
    "\n",
    "# create vqaEval object by taking vqa and vqaRes\n",
    "vqaEval = VQAEval(vqa, vqaRes, n=2)   #n is precision of accuracy (number of places after decimal), default is 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate results\n",
    "\"\"\"\n",
    "If you have a list of question ids on which you would like to evaluate your results, pass it as a list to below function\n",
    "By default it uses all the question ids in annotation file\n",
    "\"\"\"\n",
    "vqaEval.evaluate() \n",
    "\n",
    "# print accuracies\n",
    "print (\"\\n\")\n",
    "print (\"Overall Accuracy is: {:.2f}\\n\".format(vqaEval.accuracy['overall']))\n",
    "print (\"Per Question Type Accuracy is the following:\")\n",
    "for quesType in vqaEval.accuracy['perQuestionType']:\n",
    "\tprint (\"{} : {:.2f}\".format(quesType, vqaEval.accuracy['perQuestionType'][quesType]))\n",
    "print (\"\\n\")\n",
    "print (\"Per Answer Type Accuracy is the following:\")\n",
    "for ansType in vqaEval.accuracy['perAnswerType']:\n",
    "        print (\"{} : {:.2f}\".format(ansType, vqaEval.accuracy['perAnswerType'][ansType]))\n",
    "print (\"\\n\")\n",
    "# demo how to use evalQA to retrieve low score result\n",
    "evals = [quesId for quesId in vqaEval.evalQA if vqaEval.evalQA[quesId]<35]   #35 is per question percentage accuracy\n",
    "if len(evals) > 0:\n",
    "\tprint ('ground truth answers')\n",
    "\trandomEval = random.choice(evals)\n",
    "\trandomAnn = vqa.loadQA(randomEval)\n",
    "\tvqa.showQA(randomAnn)\n",
    "\n",
    "\tprint ('\\n')\n",
    "\tprint ('generated answer (accuracy {:.2f})'.format(vqaEval.evalQA[randomEval]))\n",
    "\tann = vqaRes.loadQA(randomEval)[0]\n",
    "\tprint (\"Answer:   {}\\n\" .format(ann['answer']))\n",
    "\n",
    "\timgId = randomAnn[0]['image_id']\n",
    "\timgFilename = 'COCO_' + dataSubType + '_'+ str(imgId).zfill(12) + '.jpg'\n",
    "\tif os.path.isfile(imgDir + imgFilename):\n",
    "\t\tI = io.imread(imgDir + imgFilename)\n",
    "\t\tplt.imshow(I)\n",
    "\t\tplt.axis('off')\n",
    "\t\tplt.show()\n",
    "\n",
    "# plot accuracy for various question types\n",
    "plt.bar(range(len(vqaEval.accuracy['perQuestionType'])), vqaEval.accuracy['perQuestionType'].values(), align='center')\n",
    "plt.xticks(range(len(vqaEval.accuracy['perQuestionType'])), vqaEval.accuracy['perQuestionType'].keys(), rotation='0',fontsize=10)\n",
    "plt.title('Per Question Type Accuracy', fontsize=10)\n",
    "plt.xlabel('Question Types', fontsize=10)\n",
    "plt.ylabel('Accuracy', fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# save evaluation results to ./Results folder\n",
    "json.dump(vqaEval.accuracy,     open(accuracyFile,     'w'))\n",
    "json.dump(vqaEval.evalQA,       open(evalQAFile,       'w'))\n",
    "json.dump(vqaEval.evalQuesType, open(evalQuesTypeFile, 'w'))\n",
    "json.dump(vqaEval.evalAnsType,  open(evalAnsTypeFile,  'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myenv] *",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
